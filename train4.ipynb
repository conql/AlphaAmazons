{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from model import NeuralNet\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from pprint import pprint\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchDataSet(Dataset):\n",
    "    def __init__(self, data: dict):\n",
    "        self.data = data\n",
    "        self.keys = list(data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        obj = self.data[self.keys[index]]\n",
    "        board = torch.tensor(obj[0], dtype=torch.int8)\n",
    "        answer = torch.tensor(obj[1], dtype=torch.int8)\n",
    "        value = torch.tensor(obj[2], dtype=torch.float)\n",
    "        return (board, answer, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input = []\n",
    "    policies = []\n",
    "    values = []\n",
    "    # pprint(batch)\n",
    "    for i, (board, answer, value) in enumerate(batch):\n",
    "        player = board[answer[1]][answer[0]]\n",
    "        if player != 1 and player != 2:\n",
    "            raise Exception(\"Invalid player: {}\".format(player))\n",
    "\n",
    "        # pprint(board)\n",
    "        # pprint(answer)\n",
    "\n",
    "        input.append(NeuralNet.process_input(board, player, 0))\n",
    "        input.append(NeuralNet.process_input(board, player, 1, answer[0], answer[1]))\n",
    "\n",
    "        n_board = board.clone()\n",
    "        n_board[answer[1], answer[0]] = 0\n",
    "        n_board[answer[3], answer[2]] = player\n",
    "\n",
    "        input.append(NeuralNet.process_input(n_board, player, 2, answer[2], answer[3]))\n",
    "\n",
    "        p1, v1 = NeuralNet.process_output(value, answer[0], answer[1], board, player, 0)\n",
    "\n",
    "        p2, v2 = NeuralNet.process_output(\n",
    "            value, answer[2], answer[3], board, player, 1, answer[0], answer[1]\n",
    "        )\n",
    "\n",
    "        p3, v3 = NeuralNet.process_output(\n",
    "            value,\n",
    "            answer[4],\n",
    "            answer[5],\n",
    "            n_board,\n",
    "            player,\n",
    "            2,\n",
    "            answer[2],\n",
    "            answer[3],\n",
    "        )\n",
    "\n",
    "        policies.append(p1)\n",
    "        policies.append(p2)\n",
    "        policies.append(p3)\n",
    "\n",
    "        values.append(v1)\n",
    "        values.append(v2)\n",
    "        values.append(v3)\n",
    "\n",
    "    # convert to torch tensor\n",
    "    input = torch.stack(input)\n",
    "    policies = torch.stack(policies)\n",
    "    values = torch.stack(values).view(-1, 1)\n",
    "    return input, (policies, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    input_paths = glob(path, recursive=False)\n",
    "\n",
    "    data = {}\n",
    "    for path in tqdm(input_paths, desc=\"Loading data\", unit=\"file\"):\n",
    "        data = {**data, **pickle.load(open(path, \"rb\"))}\n",
    "\n",
    "    return MatchDataSet(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huang\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\neptune\\common\\warnings.py:62: NeptuneWarning: To avoid unintended consumption of logging hours during interactive sessions, the following monitoring options are disabled unless set to 'True' when initializing the run: 'capture_stdout', 'capture_stderr', and 'capture_hardware_metrics'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/conql/Amazons/e/AM-83\n"
     ]
    }
   ],
   "source": [
    "model_name = \"data/resnet_01_40x128.pt\"\n",
    "data_path = \"data/augment3/train/*.pickle\"\n",
    "\n",
    "config.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 29/29 [00:24<00:00,  1.17file/s]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_dataset = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "net = NeuralNet()\n",
    "if os.path.exists(model_name):\n",
    "    net.load(path=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   0%|          | 0/90626 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Index put requires the source and destination dtypes match, got Char for the destination and Float for the source.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m out_pi, out_v \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mmodel(input_data)\n\u001b[0;32m     34\u001b[0m \u001b[39m# Some samples have no value, so we need to replace it with the predicted value\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m target_vs[target_vs \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m] \u001b[39m=\u001b[39m out_v[target_vs \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[0;32m     37\u001b[0m p_loss \u001b[39m=\u001b[39m loss_pi(out_pi, target_pi)\n\u001b[0;32m     38\u001b[0m v_loss \u001b[39m=\u001b[39m loss_v(out_v, target_vs)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Index put requires the source and destination dtypes match, got Char for the destination and Float for the source."
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "optimizer = torch.optim.Adam(net.model.parameters(), lr=config.lr)\n",
    "loss_pi = nn.CrossEntropyLoss()\n",
    "loss_v = nn.MSELoss()\n",
    "net.model.train()\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    train_correct = 0\n",
    "    train_actual_correct = 0\n",
    "    train_ploss_sum = 0\n",
    "    train_vloss_sum = 0\n",
    "    train_count = 0\n",
    "\n",
    "    for i, (X, Y) in tqdm(\n",
    "        enumerate(train_loader), total=len(train_loader), desc=\"Training model\"\n",
    "    ):\n",
    "        input_data = X\n",
    "        target_pi, target_vs = Y\n",
    "        sample_size = input_data.shape[0]\n",
    "\n",
    "        if config.use_gpu:\n",
    "            input_data, target_pi, target_vs = (\n",
    "                input_data.contiguous().cuda(),\n",
    "                target_pi.contiguous().cuda(),\n",
    "                target_vs.contiguous().cuda(),\n",
    "            )\n",
    "\n",
    "        # predict\n",
    "        out_pi, out_v = net.model(input_data)\n",
    "\n",
    "        # Some samples have no value, so we need to replace it with the predicted value\n",
    "        target_vs[target_vs == 0] = out_v[target_vs == 0]\n",
    "\n",
    "        p_loss = loss_pi(out_pi, target_pi)\n",
    "        v_loss = loss_v(out_v, target_vs)\n",
    "\n",
    "        total_loss = p_loss + v_loss\n",
    "\n",
    "        # update loss\n",
    "        train_ploss_sum += p_loss.item() * sample_size\n",
    "        train_vloss_sum += v_loss.item() * sample_size\n",
    "\n",
    "        corrects = out_pi.view(sample_size, -1).argmax(1) == target_pi.view(\n",
    "            sample_size, -1\n",
    "        ).argmax(1)\n",
    "\n",
    "        # update correct\n",
    "        train_correct += corrects.sum().item()\n",
    "\n",
    "        # calculate actual correct: 3 consecutive corrects\n",
    "        ac = 0\n",
    "        for j in range(0, sample_size, 3):\n",
    "            if corrects[j] and corrects[j + 1] and corrects[j + 2]:\n",
    "                ac += 1\n",
    "        train_actual_correct += ac\n",
    "\n",
    "        train_count += sample_size\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update log\n",
    "        if i % 10 == 0:\n",
    "            config.run[\"policy loss\"].append(train_ploss_sum / train_count)\n",
    "            config.run[\"value loss\"].append(train_vloss_sum / train_count)\n",
    "            config.run[\"accuracy\"].append(train_correct / train_count)\n",
    "            config.run[\"actual accuracy\"].append(\n",
    "                train_actual_correct / (train_count / 3)\n",
    "            )\n",
    "\n",
    "            train_ploss_sum = 0\n",
    "            train_vloss_sum = 0\n",
    "            train_correct = 0\n",
    "            train_actual_correct = 0\n",
    "            train_count = 0\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            net.save(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
