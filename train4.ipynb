{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from model import NeuralNet\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from pprint import pprint\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchDataSet(Dataset):\n",
    "    def __init__(self, data: dict):\n",
    "        self.data = data\n",
    "        self.keys = list(data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        obj = self.data[self.keys[index]]\n",
    "        board = torch.tensor(obj[0], dtype=torch.int8)\n",
    "        answer = torch.tensor(obj[1], dtype=torch.int8)\n",
    "        value = torch.tensor(obj[2], dtype=torch.float)\n",
    "        return (board, answer, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input = []\n",
    "    policies = []\n",
    "    values = []\n",
    "    # pprint(batch)\n",
    "    for i, (board, answer, value) in enumerate(batch):\n",
    "        player = board[answer[1]][answer[0]]\n",
    "        if player != 1 and player != 2:\n",
    "            raise Exception(\"Invalid player: {}\".format(player))\n",
    "\n",
    "        # pprint(board)\n",
    "        # pprint(answer)\n",
    "\n",
    "        input.append(NeuralNet.process_input(board, player, 0))\n",
    "        input.append(NeuralNet.process_input(board, player, 1, answer[0], answer[1]))\n",
    "\n",
    "        n_board = board.clone()\n",
    "        n_board[answer[1], answer[0]] = 0\n",
    "        n_board[answer[3], answer[2]] = player\n",
    "\n",
    "        input.append(NeuralNet.process_input(n_board, player, 2, answer[2], answer[3]))\n",
    "\n",
    "        p1, v1 = NeuralNet.process_output(value, answer[0], answer[1], board, player, 0)\n",
    "\n",
    "        p2, v2 = NeuralNet.process_output(\n",
    "            value, answer[2], answer[3], board, player, 1, answer[0], answer[1]\n",
    "        )\n",
    "\n",
    "        p3, v3 = NeuralNet.process_output(\n",
    "            value,\n",
    "            answer[4],\n",
    "            answer[5],\n",
    "            n_board,\n",
    "            player,\n",
    "            2,\n",
    "            answer[2],\n",
    "            answer[3],\n",
    "        )\n",
    "\n",
    "        policies.append(p1)\n",
    "        policies.append(p2)\n",
    "        policies.append(p3)\n",
    "\n",
    "        values.append(v1)\n",
    "        values.append(v2)\n",
    "        values.append(v3)\n",
    "\n",
    "    # convert to torch tensor\n",
    "    input = torch.stack(input)\n",
    "    policies = torch.stack(policies)\n",
    "    values = torch.stack(values).view(-1, 1)\n",
    "    return input, (policies, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    input_paths = glob(path, recursive=False)\n",
    "\n",
    "    data = {}\n",
    "    for path in tqdm(input_paths, desc=\"Loading data\", unit=\"file\"):\n",
    "        data = {**data, **pickle.load(open(path, \"rb\"))}\n",
    "\n",
    "    return MatchDataSet(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huang\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\neptune\\common\\warnings.py:62: NeptuneWarning: To avoid unintended consumption of logging hours during interactive sessions, the following monitoring options are disabled unless set to 'True' when initializing the run: 'capture_stdout', 'capture_stderr', and 'capture_hardware_metrics'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/conql/Amazons/e/AM-84\n"
     ]
    }
   ],
   "source": [
    "model_name = \"data/resnet_01_40x128.pt\"\n",
    "data_path = \"data/augment3/train/*.pickle\"\n",
    "\n",
    "config.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 29/29 [00:22<00:00,  1.27file/s]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_dataset = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "net = NeuralNet()\n",
    "if os.path.exists(model_name):\n",
    "    net.load(path=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   1%|          | 1059/90626 [03:47<5:20:29,  4.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m train_vloss_sum \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     15\u001b[0m train_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfor\u001b[39;00m i, (X, Y) \u001b[39min\u001b[39;00m tqdm(\n\u001b[0;32m     18\u001b[0m     \u001b[39menumerate\u001b[39m(train_loader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_loader), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining model\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m ):\n\u001b[0;32m     20\u001b[0m     input_data \u001b[39m=\u001b[39m X\n\u001b[0;32m     21\u001b[0m     target_pi, target_vs \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\huang\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\huang\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\huang\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\huang\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid player: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(player))\n\u001b[0;32m     11\u001b[0m \u001b[39m# pprint(board)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# pprint(answer)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mappend(NeuralNet\u001b[39m.\u001b[39;49mprocess_input(board, player, \u001b[39m0\u001b[39;49m))\n\u001b[0;32m     15\u001b[0m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mappend(NeuralNet\u001b[39m.\u001b[39mprocess_input(board, player, \u001b[39m1\u001b[39m, answer[\u001b[39m0\u001b[39m], answer[\u001b[39m1\u001b[39m]))\n\u001b[0;32m     17\u001b[0m n_board \u001b[39m=\u001b[39m board\u001b[39m.\u001b[39mclone()\n",
      "File \u001b[1;32md:\\projects\\amazons\\model.py:118\u001b[0m, in \u001b[0;36mNeuralNet.process_input\u001b[1;34m(board, player, stage, preActionX, preActionY)\u001b[0m\n\u001b[0;32m    116\u001b[0m     board \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(board)\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39muse_gpu:\n\u001b[1;32m--> 118\u001b[0m     board \u001b[39m=\u001b[39m board\u001b[39m.\u001b[39;49mcuda()\n\u001b[0;32m    120\u001b[0m data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39m7\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m))\n\u001b[0;32m    122\u001b[0m \u001b[39massert\u001b[39;00m player \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "optimizer = torch.optim.Adam(net.model.parameters(), lr=config.lr)\n",
    "loss_pi = nn.CrossEntropyLoss()\n",
    "loss_v = nn.MSELoss()\n",
    "net.model.train()\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    train_correct = 0\n",
    "    train_actual_correct = 0\n",
    "    train_ploss_sum = 0\n",
    "    train_vloss_sum = 0\n",
    "    train_count = 0\n",
    "\n",
    "    for i, (X, Y) in tqdm(\n",
    "        enumerate(train_loader), total=len(train_loader), desc=\"Training model\"\n",
    "    ):\n",
    "        input_data = X\n",
    "        target_pi, target_vs = Y\n",
    "        sample_size = input_data.shape[0]\n",
    "\n",
    "        if config.use_gpu:\n",
    "            input_data, target_pi, target_vs = (\n",
    "                input_data.contiguous().cuda(),\n",
    "                target_pi.contiguous().cuda(),\n",
    "                target_vs.contiguous().cuda(),\n",
    "            )\n",
    "\n",
    "        # predict\n",
    "        out_pi, out_v = net.model(input_data)\n",
    "\n",
    "        # Some samples have no value, so we need to replace it with the predicted value\n",
    "        target_vs[target_vs == 0] = out_v[target_vs == 0]\n",
    "\n",
    "        p_loss = loss_pi(out_pi, target_pi)\n",
    "        v_loss = loss_v(out_v, target_vs)\n",
    "\n",
    "        total_loss = p_loss + v_loss\n",
    "\n",
    "        # update loss\n",
    "        train_ploss_sum += p_loss.item() * sample_size\n",
    "        train_vloss_sum += v_loss.item() * sample_size\n",
    "\n",
    "        corrects = out_pi.view(sample_size, -1).argmax(1) == target_pi.view(\n",
    "            sample_size, -1\n",
    "        ).argmax(1)\n",
    "\n",
    "        # update correct\n",
    "        train_correct += corrects.sum().item()\n",
    "\n",
    "        # calculate actual correct: 3 consecutive corrects\n",
    "        ac = 0\n",
    "        for j in range(0, sample_size, 3):\n",
    "            if corrects[j] and corrects[j + 1] and corrects[j + 2]:\n",
    "                ac += 1\n",
    "        train_actual_correct += ac\n",
    "\n",
    "        train_count += sample_size\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update log\n",
    "        if i % 10 == 0:\n",
    "            config.run[\"policy loss\"].append(train_ploss_sum / train_count)\n",
    "            config.run[\"value loss\"].append(train_vloss_sum / train_count)\n",
    "            config.run[\"accuracy\"].append(train_correct / train_count)\n",
    "            config.run[\"actual accuracy\"].append(\n",
    "                train_actual_correct / (train_count / 3)\n",
    "            )\n",
    "\n",
    "            train_ploss_sum = 0\n",
    "            train_vloss_sum = 0\n",
    "            train_correct = 0\n",
    "            train_actual_correct = 0\n",
    "            train_count = 0\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            net.save(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
